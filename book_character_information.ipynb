{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place to dowload book free\n",
    "#https://annas-archive.org/\n",
    "import spacy\n",
    "import ebooklib\n",
    "from ebooklib import epub  \n",
    "from html.parser import HTMLParser #(usar beutiful soup)\n",
    "from collections import defaultdict\n",
    "from spacy.lang.es import Spanish\n",
    "# En spacy, los saltos de linea los ignora, asi que supongo que para separar en parrafos o bien ingnorarlos, o bien procesarlos por separado,\n",
    "# haciendo un split por \\n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\pable\\AppData\\Local\\Temp\\ipykernel_7164\\117803313.py:2: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  book_path = \".\\Asesinato en el Orient Express -- Agatha Christie -- Hércules Poirot 10, 1934 -- cba8798523cc82925c66887e83cdaa07 -- Anna’s Archive.epub\"\n",
      "c:\\Users\\pable\\miniconda3\\envs\\main_env\\Lib\\site-packages\\ebooklib\\epub.py:1395: UserWarning: In the future version we will turn default option ignore_ncx to True.\n",
      "  warnings.warn('In the future version we will turn default option ignore_ncx to True.')\n",
      "c:\\Users\\pable\\miniconda3\\envs\\main_env\\Lib\\site-packages\\ebooklib\\epub.py:1423: FutureWarning: This search incorrectly ignores the root element, and will be fixed in a future version.  If you rely on the current behaviour, change it to './/xmlns:rootfile[@media-type]'\n",
      "  for root_file in tree.findall('//xmlns:rootfile[@media-type]', namespaces={'xmlns': NAMESPACES['CONTAINERNS']}):\n"
     ]
    }
   ],
   "source": [
    "# paths and data acquisition \n",
    "book_path = \".\\Asesinato en el Orient Express -- Agatha Christie -- Hércules Poirot 10, 1934 -- cba8798523cc82925c66887e83cdaa07 -- Anna’s Archive.epub\"\n",
    "book = epub.read_epub(book_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Asesinato en el Orient Express'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = book.get_metadata('DC', 'title')[0][0]\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to store the chapters\n",
    "class Chapter():\n",
    "    def __init__(self,chapter_name,chapter_text) -> None:\n",
    "        self.chapter_name = chapter_name\n",
    "        self.chapter_text = chapter_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered some data  : \n",
      "Encountered a start tag: p\n",
      "Encountered a start tag: span\n",
      "Encountered some data  : N\n",
      "Encountered an end tag : span\n",
      "Encountered some data  : O llevaba sombrero. Entró con la cabeza echada hacia atrás, como en un desafío. La curva de su nariz recordaba una nave surcando valiente un mar embravecido. En aquel momento, Mary Debenham estaba hermosísima.\n",
      "Encountered an end tag : p\n",
      "Encountered some data  : \n"
     ]
    }
   ],
   "source": [
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Encountered a start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"Encountered an end tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Encountered some data  :\", data)\n",
    "\n",
    "parser = MyHTMLParser()\n",
    "parser.feed('&#13;<p class=\"asangre salto35\"><span class=\"letra\">N</span>O llevaba sombrero. Entró con la cabeza echada hacia atrás, como en un desafío. La curva de su nariz recordaba una nave surcando valiente un mar embravecido. En aquel momento, Mary Debenham estaba hermosísima.</p>&#13;&#13;')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#13;\n",
    "  <h2 class=\"oculto\" title=\"7. La identidad de Mary Debenham\"/>&#13;\n",
    "&#13;\n",
    "  <div class=\"capitulo centrado\">&#13;\n",
    "    <p class=\"salto30\">7</p>&#13;\n",
    "&#13;\n",
    "    <p class=\"salto05\">LA IDENTIDAD DE MARY DEBENHAM</p>&#13;\n",
    "  </div>&#13;\n",
    "&#13;\n",
    "  <p class=\"asangre salto35\"><span class=\"letra\">N</span>O llevaba sombrero. Entró con la cabeza echada hacia atrás, como en un desafío. La curva de su nariz recordaba una nave surcando valiente un mar embravecido. En aquel momento, Mary Debenham estaba hermosísima.</p>&#13;\n",
    "&#13;\n",
    "  <p>Su mirada se posó en Arbuthnot un instante…, sólo un instante.</p>&#13;\n",
    "&#13;\n",
    "  <p>—Deseaba preguntarle, señorita, por qué nos mintió usted esta mañana.</p>&#13;\n",
    "&#13;\n",
    "  <p>—¿Mentirle yo? No sé a lo que se refiere.</p>&#13;\n",
    "&#13;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTMLFilter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m bodyContent \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mget_body_content()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m----> 6\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mHTMLFilter\u001b[49m()\n\u001b[0;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mfeed(bodyContent)\n\u001b[0;32m      8\u001b[0m content \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTMLFilter' is not defined"
     ]
    }
   ],
   "source": [
    "# function to get an orderer array of Chapters until a selected chapter\n",
    "for item in book.get_items_of_type(ebooklib.ITEM_DOCUMENT):\n",
    "    if item.is_chapter():\n",
    "        bodyContent = item.get_body_content().decode()\n",
    "        print()\n",
    "        f = HTMLFilter()\n",
    "        f.feed(bodyContent)\n",
    "        content += f.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_adj_for_person(person_name,nlp_sentence):\n",
    "    '''\n",
    "    return all the adjectives in the sentence that are reffering to the person_name \n",
    "    '''\n",
    "    adjectives = []\n",
    "    for token in nlp_sentence:\n",
    "\n",
    "        # If we find a character in the phrase or a NOUN, we get it\n",
    "        if token.pos_  == \"PROPN\":\n",
    "            character = token.text\n",
    "        \n",
    "        elif token.pos_  == \"NOUN\":\n",
    "            character = None\n",
    "\n",
    "        \n",
    "        # If the token is an adjective, and its refering the character we want, we save them\n",
    "        if character == person_name and token.pos_ == 'ADJ':\n",
    "            adjectives.append(token.text)\n",
    "\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alto', 'guapo']\n",
      "['bajo', 'feo']\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "text = \"Pablo es alto y guapo y tiene una silla bonita, mientras que Juan es bajo y feo y come muy mal\"\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "# Preprocess the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Initialize containers to store character information\n",
    "character_info = defaultdict(lambda: {\n",
    "    \"description\": [],\n",
    "    \"sentences\": [],\n",
    "    \"mentions\": 0,\n",
    "    \"relationships\": defaultdict(int),\n",
    "    \"actions\": []\n",
    "})\n",
    "\n",
    "# Iterate through the sentences to extract relevant information\n",
    "for sent in doc.sents:\n",
    "    sent_doc = nlp(sent.text)\n",
    "    characters_in_sentence = []\n",
    "    \n",
    "    # We analyze the sentence token by token\n",
    "    for token in sent_doc:\n",
    "        # if there is an entity of type person we add all adjectives in the sentence as the description of the entity, until we find another person\n",
    "        if token.ent_type_ == \"PER\":\n",
    "            character_name = token.text\n",
    "            \n",
    "            # Add the sentence to the sentences list of that character\n",
    "            character_info[character_name][\"sentences\"] += sent.text\n",
    "\n",
    "            # Add the adjectives to the adjectives list of that character\n",
    "            character_info[character_name][\"description\"] += search_adj_for_person(character_name,sent_doc)\n",
    "\n",
    "            # Add the thigs he migth tell or say to the adjectives list of that character\n",
    "\n",
    "            if token.dep_ == \"ROOT\" and token.lemma_ in [\"do\", \"say\", \"act\", \"think\", \"feel\"]:\n",
    "                # Extract actions, e.g., \"John ran\", \"Sarah shouted\"\n",
    "                for char in characters_in_sentence:\n",
    "                    character_info[char][\"actions\"].append(sent.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(character_info['Pablo'][\"description\"])\n",
    "print(character_info['Juan'][\"description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "-¿Qué sintió al saber lo que había hecho?-preguntó Armstrong-.¿No lo lamentó?¿No se reprochó su conducta?     \n",
      "----\n",
      "Lombrad exclamó:-¿Yo?\n",
      "----\n",
      "No tenía nada que reprocharme.\n",
      "defaultdict(<function <lambda> at 0x0000027CEBB914E0>, {'Lombrad': {'description': [], 'sentences': ['L', 'o', 'm', 'b', 'r', 'a', 'd', ' ', 'e', 'x', 'c', 'l', 'a', 'm', 'ó', ':', '-', '¿', 'Y', 'o', '?'], 'mentions': 0, 'relationships': defaultdict(<class 'int'>, {}), 'actions': []}})\n"
     ]
    }
   ],
   "source": [
    "agata_text = \"-¿Qué sintió al saber lo que había hecho?-preguntó Armstrong-.¿No lo lamentó?¿No se reprochó su conducta? \\\n",
    "    Lombrad exclamó:-¿Yo? No tenía nada que reprocharme.\"\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "# Preprocess the text\n",
    "doc = nlp(agata_text)\n",
    "\n",
    "# Initialize containers to store character information\n",
    "character_info = defaultdict(lambda: {\n",
    "    \"description\": [],\n",
    "    \"sentences\": [],\n",
    "    \"mentions\": 0,\n",
    "    \"relationships\": defaultdict(int),\n",
    "    \"actions\": []\n",
    "})\n",
    "\n",
    "# Iterate through the sentences to extract relevant information\n",
    "for sent in doc.sents:\n",
    "    print(\"----\")\n",
    "    print(sent)\n",
    "    sent_doc = nlp(sent.text)\n",
    "    characters_in_sentence = []\n",
    "    \n",
    "    # We analyze the sentence token by token\n",
    "    for token in sent_doc:\n",
    "        # if there is an entity of type person we add all adjectives in the sentence as the description of the entity, until we find another person\n",
    "        if token.ent_type_ == \"PER\":\n",
    "            character_name = token.text\n",
    "            \n",
    "            # Add the sentence to the sentences list of that character\n",
    "            character_info[character_name][\"sentences\"] += sent.text\n",
    "\n",
    "            # Add the adjectives to the adjectives list of that character\n",
    "            character_info[character_name][\"description\"] += search_adj_for_person(character_name,sent_doc)\n",
    "\n",
    "\n",
    "\n",
    "print(character_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'root'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ROOT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumir info\n",
    "from transformers import pipeline\n",
    "\n",
    "# Cargar un pipeline de resumen\n",
    "resumir = pipeline('summarization')\n",
    "\n",
    "# Crear resumenes cortos de las oraciones donde aparece cada personaje\n",
    "for personaje, oraciones in info_personajes.items():\n",
    "    print(f\"Personaje: {personaje}\")\n",
    "    texto_unido = \" \".join(oraciones)\n",
    "    resumen = resumir(texto_unido[:1000], max_length=100, min_length=30, do_sample=False)\n",
    "    print(f\"Resumen: {resumen[0]['summary_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
